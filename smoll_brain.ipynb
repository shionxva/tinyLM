{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t2G8H6WM7Hse"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters:\n",
        "\n",
        "- MAX_VOCAB : how can the model interpret\n",
        "\n",
        "- CONTEXT_WIN : maximum number of tokens the model can look at at once\n",
        "\n",
        "- EMBED_WIN : the size of each token's vector\n",
        "\n",
        "- HEADS : number of heads for multi-head attention (Each head learns a different type of relationship)\n",
        "\n",
        "- FEED_FOWARD : expand to this param size (larger) to learn more complex transformation (and later it is squished back by Dense)\n",
        "\n",
        "- TRANSFORMER_BLOCKS : blocks = deeper reasoning\n",
        "\n",
        "- BATCH_SIZE : train param\n",
        "\n",
        "- EPOCHS : train param"
      ],
      "metadata": {
        "id": "QRWGe5wYqot6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB = 1000\n",
        "CONTEXT_WIN = 20\n",
        "EMBED_WIN = 64\n",
        "HEADS = 2\n",
        "FEED_FOWARD = 128 #4x embed_win\n",
        "TRANSFORMER_BLOCKS = 2\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 70\n",
        "\n",
        "train_text = [\n",
        "\"Machine Learning is very hard\",\n",
        "\"I use Arch btw\",\n",
        "\"I need a job\",\n",
        "\"GitHub is goated\",\n",
        "\"Saber best waifu :3\",\n",
        "\"Skibidi Toilet\",\n",
        "\"This is a test\"\n",
        "]"
      ],
      "metadata": {
        "id": "_I7TLS7r9dYq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "- This is a reprocessing layer that simply split text by white space and assign it with an interger IDs\n",
        "\n",
        "- `output_sequence_length=CONTEXT_WIN` : apply \"0\" padding so that every seqs are the same length (20 in this case)\n",
        "\n",
        "# Data prep\n",
        "\n",
        "slicing the last word for input and last word for output\n",
        "-> this will then force the model to learn the connection of words and predict the next token :0"
      ],
      "metadata": {
        "id": "rYPSq7SIg9zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = layers.TextVectorization(\n",
        "    max_tokens=MAX_VOCAB,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=CONTEXT_WIN,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\"\n",
        ")\n",
        "tokenizer.adapt(train_text) #build vocab\n",
        "vocab = tokenizer.get_vocabulary() #convert to int IDs\n",
        "\n",
        "print(vocab)\n",
        "\n",
        "#next token prediction\n",
        "def prep_data(train_text):\n",
        "  seq = tokenizer(train_text)\n",
        "  x_input = seq[:, :-1]\n",
        "  y_target = seq[:, 1:]\n",
        "\n",
        "  return x_input, y_target\n",
        "\n",
        "x_train, y_train = prep_data(train_text)"
      ],
      "metadata": {
        "id": "nBxhCvtKAyK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb8343a-2c63-4914-9441-47e5ff4ffdbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', np.str_('is'), np.str_('a'), np.str_('I'), np.str_('waifu'), np.str_('very'), np.str_('use'), np.str_('test'), np.str_('need'), np.str_('job'), np.str_('hard'), np.str_('goated'), np.str_('btw'), np.str_('best'), np.str_('Toilet'), np.str_('This'), np.str_('Skibidi'), np.str_('Saber'), np.str_('Machine'), np.str_('Learning'), np.str_('GitHub'), np.str_('Arch'), np.str_(':3')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perplexity metric\n",
        "*overall it shows how many choises is the model confused between*\n",
        "- `crossentropy(true,pred)` cross entropy loss with true_labels predicted value\n",
        "- then `perplexity` = e^`crossentropy`\n",
        "\n",
        "# Token Position Embedding\n",
        "Once the text is tokenized, each token needs to be represented in a way that captures not just the token but also its meaning and position (relation to other tokens)\n",
        "\n",
        "*This is how it works visually:*\n",
        "\n",
        "```\n",
        "Input tokens:     [\"the\", \"dog\", \"runs\"]  →  IDs: [4, 27, 13]\n",
        "Token embeddings:  [row_v4,  ...  ,row_v27, ...  ,row_v13  ]  \n",
        "Position embeddings:[row_p0,  ...  ,row_p1,  ...  ,row_p2  ]  \n",
        "Output:           [v4+p0, v27+p1, v13+p2]\n",
        "```"
      ],
      "metadata": {
        "id": "uzUJbq8AkvIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(true,pred):\n",
        "  return tf.exp(tf.reduce_mean(losses.sparse_categorical_crossentropy(true,pred)))\n",
        "\n",
        "class TokenPositionEmbedding(layers.Layer):\n",
        "  def __init__(self, context_win, max_vocab, embed_dim, **kwargs) -> None:\n",
        "    super().__init__()\n",
        "    self.token_embed = layers.Embedding(input_dim = max_vocab, output_dim=embed_dim)\n",
        "    self.position_embed = layers.Embedding(input_dim = context_win, output_dim=embed_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    context_win = tf.shape(x)[-1]\n",
        "    positions = self.position_embed(tf.range(start=0, limit=context_win, delta=1))\n",
        "    return self.token_embed(x) + positions\n"
      ],
      "metadata": {
        "id": "IBl4ngLEN1mF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer blocks\n",
        "Overall tructure:\n",
        "```\n",
        "Input -> Multi-head attention -> Add Residual -> LayerNorm\n",
        "-> Feed Forward ->Add Residual -> LayerNorm -> Output\n",
        "```\n",
        "- `multi-head attention`: each head works on an equal slice of the embedding.\n",
        "\n",
        "- `feed forward network` is a small 2 layers network:\n",
        "\n",
        "It first expands the dimension to feed_forward, applies ReLU, then squishes back down to embed_dim\n",
        "\n",
        "- `layer norm and dropout`: norm stabilizes training and dropout randomly zeros out 10% of values during training to prevent overfitting.\n",
        "\n",
        "*notes: self-attention is basically seeing how other words are relevant to the current word*"
      ],
      "metadata": {
        "id": "jUtlF3qepJy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim, heads, feed_forward, **kwargs) -> None:\n",
        "    super().__init__()\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=heads, key_dim=embed_dim // heads)\n",
        "    self.feed_foward_net = models.Sequential([layers.Dense(feed_forward, activation=\"relu\"), layers.Dense(embed_dim)])\n",
        "    self.norm1 = layers.LayerNormalization(epsilon=1e-4)\n",
        "    self.norm2 = layers.LayerNormalization(epsilon=1e-4)\n",
        "    self.drop1 = layers.Dropout(0.1)\n",
        "    self.drop2 = layers.Dropout(0.1)\n",
        "\n",
        "  def call(self, inputs, training = False):\n",
        "    #Self Attention                   query    key/value\n",
        "    attention_output = self.attention(inputs, inputs, use_causal_mask=True)\n",
        "    attention_output = self.drop1(attention_output, training=training)\n",
        "    output = self.norm1(inputs + attention_output)\n",
        "\n",
        "    #Feed Forward Network\n",
        "    feed_forward_output = self.feed_foward_net(output)\n",
        "    feed_forward_output = self.drop2(feed_forward_output, training=training)\n",
        "\n",
        "    return self.norm2(output + feed_forward_output)"
      ],
      "metadata": {
        "id": "3HPClzxtTb6A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# miniLM\n",
        "\n",
        "Now we put everything into this `miniLM` class + a text generation function\n",
        "\n",
        "a little overview of our smoll brain:\n",
        "- `embedding layer` converts token IDs into vectors\n",
        "- `transformer blocks` to process the sequence\n",
        "- `dense out` gives us the logits or \"score\" for each word in the vocab\n",
        "\n",
        "text generation process:\n",
        "- tokenize input (also remove paddings)\n",
        "- ran the loop for a fixed length\n",
        "  - `tokens[-CONTEXT_WIN:]` only have context of the previous word\n",
        "  - run the model -> get logits and apply temperature\n",
        "  - apply top_k to get the top k-th prediction\n",
        "  - softmax to convert in into probability\n",
        "  -> randomly pick a word"
      ],
      "metadata": {
        "id": "4tXDW-SjwueO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class miniLM(models.Model):\n",
        "  def __init__(self, context_win, max_vocab, embed_dim, heads, feed_forward, blocks, **kwargs) -> None:\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_layer = TokenPositionEmbedding(context_win, max_vocab, embed_dim)\n",
        "    self.transformer_blocks = [TransformerBlock(embed_dim, heads, feed_forward) for _ in range(blocks)]\n",
        "    self.dense_out = layers.Dense(max_vocab)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "      x = self.embed_layer(inputs)            # (batch, seq_len, embed_dim)\n",
        "      for block in self.transformer_blocks:\n",
        "        x = block(x, training=training)       # (batch, seq_len, embed_dim)\n",
        "      return self.dense_out(x)                # (batch, seq_len, max_vocab)\n",
        "\n",
        "\n",
        "  def gen(model, prompt, length = 6, temperature= 1.0, top_k = 5):\n",
        "    input_tensor = tokenizer([prompt])\n",
        "    tokens = [token for token in input_tensor.numpy()[0] if token != 0]\n",
        "\n",
        "    gen_text = prompt\n",
        "    for _ in range(length):\n",
        "      context_token = tokens[-CONTEXT_WIN:]\n",
        "      input_data = tf.convert_to_tensor([context_token])\n",
        "\n",
        "      preds = model(input_data, training=False)\n",
        "      next_logits = preds[0, -1, :]\n",
        "      next_logits /= (temperature + 1e-7)\n",
        "      top_val,  top_idx = tf.math.top_k(next_logits, k=top_k)\n",
        "      top_prob = tf.nn.softmax(top_val).numpy()\n",
        "\n",
        "      next_idx = np.random.choice(top_idx.numpy(), p=top_prob)\n",
        "      if next_idx == 0 and len(tokens) >0:\n",
        "        next_idx = top_idx.numpy()[1]\n",
        "\n",
        "      tokens.append(next_idx)\n",
        "      gen_text += \" \" + vocab[next_idx]\n",
        "\n",
        "    return gen_text"
      ],
      "metadata": {
        "id": "e5hpAgHRdUWP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skibidi_model = miniLM(CONTEXT_WIN, len(vocab), EMBED_WIN, HEADS, FEED_FOWARD, TRANSFORMER_BLOCKS)\n",
        "skibidi_model.compile(optimizer=\"adam\", loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[perplexity])\n",
        "skibidi_model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOXj-7pYg306",
        "outputId": "ab17a803-d1f5-49d1-b50c-58bca51bccdc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 3.8777 - perplexity: 20790.5938\n",
            "Epoch 2/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.6777 - perplexity: 21.4380\n",
            "Epoch 3/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.7580 - perplexity: 14.5236\n",
            "Epoch 4/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.5571 - perplexity: 11.4399\n",
            "Epoch 5/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.4928 - perplexity: 9.2738\n",
            "Epoch 6/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.4532 - perplexity: 7.7594\n",
            "Epoch 7/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.4088 - perplexity: 6.4648\n",
            "Epoch 8/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3867 - perplexity: 6.3210\n",
            "Epoch 9/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.3211 - perplexity: 5.8677\n",
            "Epoch 10/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2896 - perplexity: 5.6808\n",
            "Epoch 11/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2373 - perplexity: 5.5228\n",
            "Epoch 12/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2146 - perplexity: 5.3244\n",
            "Epoch 13/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.1971 - perplexity: 5.2209\n",
            "Epoch 14/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1648 - perplexity: 5.1219\n",
            "Epoch 15/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1444 - perplexity: 5.0376\n",
            "Epoch 16/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1417 - perplexity: 4.8761\n",
            "Epoch 17/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.1250 - perplexity: 4.6066\n",
            "Epoch 18/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.1075 - perplexity: 4.5443\n",
            "Epoch 19/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1000 - perplexity: 4.5551\n",
            "Epoch 20/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0879 - perplexity: 4.4289\n",
            "Epoch 21/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0789 - perplexity: 4.3396\n",
            "Epoch 22/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0716 - perplexity: 4.1988\n",
            "Epoch 23/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0628 - perplexity: 4.1153\n",
            "Epoch 24/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0644 - perplexity: 4.1268\n",
            "Epoch 25/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0568 - perplexity: 4.0426\n",
            "Epoch 26/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0546 - perplexity: 4.0082\n",
            "Epoch 27/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0528 - perplexity: 3.9372\n",
            "Epoch 28/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0548 - perplexity: 3.9156\n",
            "Epoch 29/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0492 - perplexity: 3.7745\n",
            "Epoch 30/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0421 - perplexity: 3.7412\n",
            "Epoch 31/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0462 - perplexity: 3.7216\n",
            "Epoch 32/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0485 - perplexity: 3.6632\n",
            "Epoch 33/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0450 - perplexity: 3.6969\n",
            "Epoch 34/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0409 - perplexity: 3.6126\n",
            "Epoch 35/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0406 - perplexity: 3.5315\n",
            "Epoch 36/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0337 - perplexity: 3.5266\n",
            "Epoch 37/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0322 - perplexity: 3.5855\n",
            "Epoch 38/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0360 - perplexity: 3.4311\n",
            "Epoch 39/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0383 - perplexity: 3.5036\n",
            "Epoch 40/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0316 - perplexity: 3.4689\n",
            "Epoch 41/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0286 - perplexity: 3.4989\n",
            "Epoch 42/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0297 - perplexity: 3.4372\n",
            "Epoch 43/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0257 - perplexity: 3.4283\n",
            "Epoch 44/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0278 - perplexity: 3.4321\n",
            "Epoch 45/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0249 - perplexity: 3.4236\n",
            "Epoch 46/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0324 - perplexity: 3.4014\n",
            "Epoch 47/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0283 - perplexity: 3.4409\n",
            "Epoch 48/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0268 - perplexity: 3.4042\n",
            "Epoch 49/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0211 - perplexity: 3.4838\n",
            "Epoch 50/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0234 - perplexity: 3.4051\n",
            "Epoch 51/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0234 - perplexity: 3.3889\n",
            "Epoch 52/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0242 - perplexity: 3.3924\n",
            "Epoch 53/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0252 - perplexity: 3.3463\n",
            "Epoch 54/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0220 - perplexity: 3.3518\n",
            "Epoch 55/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0227 - perplexity: 3.3071\n",
            "Epoch 56/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0238 - perplexity: 3.3223\n",
            "Epoch 57/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0190 - perplexity: 3.3394\n",
            "Epoch 58/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0199 - perplexity: 3.4103\n",
            "Epoch 59/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0191 - perplexity: 3.2339\n",
            "Epoch 60/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0225 - perplexity: 3.2409\n",
            "Epoch 61/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0200 - perplexity: 3.2557\n",
            "Epoch 62/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0231 - perplexity: 3.2185\n",
            "Epoch 63/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0183 - perplexity: 3.2479\n",
            "Epoch 64/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0182 - perplexity: 3.3001\n",
            "Epoch 65/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0149 - perplexity: 3.3207\n",
            "Epoch 66/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0206 - perplexity: 3.2680\n",
            "Epoch 67/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0192 - perplexity: 3.2230\n",
            "Epoch 68/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0178 - perplexity: 3.1813\n",
            "Epoch 69/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0183 - perplexity: 3.2075\n",
            "Epoch 70/70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0211 - perplexity: 3.2540\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c91105cbd10>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = input(\"Enter a prompt: \")\n",
        "print(skibidi_model.gen(test_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-006JfGhzD4",
        "outputId": "a9cd6d3e-5c8b-4e4f-80ff-79ffe4b94871"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a prompt: i like\n",
            "i like waifu :3 Machine Learning is very\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save full model, weight and tokenized vocab\n",
        "skibidi_model.save(\"skibidi_model.keras\")\n",
        "skibidi_model.save_weights(\"skibidi_model.weights.h5\")\n",
        "import pickle\n",
        "with open(\"vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab, f)"
      ],
      "metadata": {
        "id": "cA7wkwmD7lYM"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}